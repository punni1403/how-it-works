# ğ–ğ¡ğšğ­ ğğğ­ğğ«ğ¦ğ¢ğ§ğğ¬ ğ¢ğŸ ğ­ğ¡ğ ğŠğ®ğ›ğğ«ğ§ğğ­ğğ¬ ğğ¨ğğğ¬ ğšğ«ğ ğ¡ğğšğ¥ğ­ğ¡ğ²

In a nutshell, Kubernetes checks that a kubelet has registered to the API server that matches the metadata.name field of the Node. If the node is healthy (i.e. all necessary services are running), then it is eligible to run a Pod. Otherwise, that node is ignored for any cluster activity until it becomes healthy.

Often during initial set-up or upgrades, there comes a situation whether Kuberenetes nodes report "Not Ready" or sometimes "Unknown" state. What does â€œreadyâ€ even mean for k8s nodes?

The kubelet on each node is tasked with sending the control plane, specifically the ğ’ğ’ğ’…ğ’†-ğ’ğ’Šğ’‡ğ’†ğ’„ğ’šğ’„ğ’ğ’†-ğ’„ğ’ğ’ğ’•ğ’“ğ’ğ’ğ’ğ’†ğ’“, running a list of checks that determines:
â¡ Whether the container runtime network is ready
â¡ Whether the CSI provider on the node is ready
â¡ Incomplete container runtime status check 
â¡ Status of the Container runtime & pod lifecycle-event generator
â¡ The node is shutting down 
â¡ Missing CPU, memory, or max pods capacities

Based on the results, the node-lifecycle-controller takes this state and sets the nodeâ€™s Ready condition to one of the following statuses:
âœ… ğ‘»ğ’“ğ’–ğ’† - The node is ready because all of the kubelet checks from the node came back green/healthy/no errors
âœ… ğ‘­ğ’‚ğ’ğ’”ğ’†- The node isnâ€™t ready because one or more of the kubelet checks came back red/unhealthy/with errors
âœ… ğ‘¼ğ’ğ’Œğ’ğ’ğ’˜ğ’ - node-lifecycle-controller hasnâ€™t heard from the nodeâ€™s kubelet in node-monitor-grace-period time


![alt text](what_determines_if_the_kubernetes_nodes_are_healthy.jpg "ğ–ğ¡ğšğ­ ğğğ­ğğ«ğ¦ğ¢ğ§ğğ¬ ğ¢ğŸ ğ­ğ¡ğ ğŠğ®ğ›ğğ«ğ§ğğ­ğğ¬ ğğ¨ğğğ¬ ğšğ«ğ ğ¡ğğšğ¥ğ­ğ¡ğ²")
