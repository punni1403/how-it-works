## Throughput & Latency

𝗧𝗵𝗿𝗼𝘂𝗴𝗵𝗽𝘂𝘁 is the amount of data a system processes within a given period.
 
It's measured in transactions per second (TPS), requests per second (RPS), or data units per second. 

High throughput means the system can handle more requests in less time.

𝗧𝗵𝗶𝗻𝗴𝘀 𝘁𝗵𝗮𝘁 𝘄𝗶𝗹𝗹 𝗵𝗲𝗹𝗽 𝘆𝗼𝘂 𝘁𝗼 𝗶𝗻𝗰𝗿𝗲𝗮𝘀𝗲 𝗧𝗵𝗿𝗼𝘂𝗴𝗵𝗽𝘂𝘁:

- Horizontal Scaling (Adding More Machines)
- Load Balancing
- Asynchronous Processing or message queues


𝗟𝗮𝘁𝗲𝗻𝗰𝘆 is the time taken to process a single operation or request.

It's usually measured in milliseconds or microseconds.

Low latency is essential in systems with critical response time, like high-frequency trading systems.

𝗧𝗵𝗶𝗻𝗴𝘀 𝘁𝗵𝗮𝘁 𝘄𝗶𝗹𝗹 𝗵𝗲𝗹𝗽 𝘆𝗼𝘂 𝗿𝗲𝗱𝘂𝗰𝗲 𝗟𝗮𝘁𝗲𝗻𝗰𝘆: 

- Optimizing Code
- Using Faster Storage
- Database Performance Tuning
- Caching


𝗕𝗮𝗹𝗮𝗻𝗰𝗶𝗻𝗴 𝘁𝗵𝗲𝘀𝗲 𝘁𝘄𝗼 𝗼𝗳𝘁𝗲𝗻 𝗶𝗻𝘃𝗼𝗹𝘃𝗲𝘀 𝘁𝗿𝗮𝗱𝗲-𝗼𝗳𝗳𝘀. 

Asynchronous processing can increase throughput but might add to latency for individual tasks. 

Extensive caching can reduce latency but requires more memory resources if not managed.

There's often a trade-off between maximizing throughput and minimizing latency.

![Throughput and Latency.](Throughput_and_Latency.jpg "Throughput and Latency")

